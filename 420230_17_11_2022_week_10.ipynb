{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tf4pvpR35Tt",
        "outputId": "02eacb62-f057-4ea3-909d-6432add67151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': {'B': True}, 'evidence': {'J': True, 'M': True}} probability:  0.6\n",
            "{'query': {'B': True}, 'evidence': {'J': True, 'M': True}} probability:  0.288\n",
            "{'query': {'B': True}, 'evidence': {'J': True, 'M': True}} probability:  0.20874751491053675\n",
            "{'query': {'B': True}, 'evidence': {'J': True, 'M': True}} probability:  0.0\n"
          ]
        }
      ],
      "source": [
        "import itertools, copy, random\n",
        "\n",
        "class RandomVariable:\n",
        "    '''\n",
        "    Class to hold metadata about each random variable.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, name, denotion, values, parents, children, cpt, topo_index):\n",
        "        self.name = name\n",
        "        self.denotion = denotion\n",
        "        self.values = values\n",
        "        self.parents = parents\n",
        "        self.children = children\n",
        "        self.cpt = cpt\n",
        "        self.topo_index = topo_index\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.name\n",
        "    \n",
        "    def call_build_distribution(self):\n",
        "        if self.parents:\n",
        "            dis = self.build_distribution(self.parents, 0)\n",
        "            cpt = {self.denotion:{value:copy.deepcopy(dis) for value in self.values}}\n",
        "        else:\n",
        "            cpt = {self.denotion:{value:0 for value in self.values}}\n",
        "        self.cpt = cpt\n",
        "        return \n",
        "    \n",
        "    def build_distribution(self, rvars, index):\n",
        "        if (index+1) == len(rvars):\n",
        "            iter_dis = {rvars[index].denotion:{value:0 for value in rvars[index].values}}\n",
        "            return iter_dis\n",
        "        else:\n",
        "            next_iter_dict = self.build_distribution(rvars, index+1)\n",
        "            iter_dis = {rvars[index].denotion:{value:copy.deepcopy(next_iter_dict) for value in rvars[index].values}}\n",
        "            return iter_dis\n",
        "\n",
        "    def display_node(self):\n",
        "        print('Name: ', self.name)\n",
        "        print('Denoted by: ', self.denotion)\n",
        "        print('Values: ', self.values)\n",
        "        print('Parents: ', [parent.name for parent in self.parents or []])\n",
        "        print('Children: ', [child.name for child in self.children or []])\n",
        "        print(\"CPT: \")\n",
        "        print(self.cpt)\n",
        "\n",
        "\n",
        "class BayesianNetwork():\n",
        "    '''\n",
        "    Bayesian network class to hold the network of nodes.\n",
        "    It extends the itertools class to make use of combinations method by overriding it.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, random_vars):\n",
        "        self.random_vars = random_vars\n",
        "\n",
        "    \n",
        "    def combinations(self, n, item_list):\n",
        "        single_list = [__ for _ in item_list for __ in _]\n",
        "        combs = itertools.combinations(single_list, n)\n",
        "        valid_combs = []\n",
        "        for comb in list(combs):\n",
        "            flag = False\n",
        "            for sub_list in item_list:\n",
        "                for comb_2 in itertools.combinations(comb, 2):\n",
        "                    if all(x in sub_list for x in comb_2):\n",
        "                        flag = True\n",
        "                        break\n",
        "                if flag:\n",
        "                    break\n",
        "            if not flag:\n",
        "                valid_combs.append(dict(comb))\n",
        "        return valid_combs\n",
        "    \n",
        "    def build_distribution(self, rvars, index):\n",
        "        if (index+1) == len(rvars):\n",
        "            iter_dis = {rvars[index].denotion:{value:0 for value in rvars[index].values}}\n",
        "            return iter_dis\n",
        "        else:\n",
        "            next_iter_dict = self.build_distribution(rvars, index+1)\n",
        "            iter_dis = {rvars[index].denotion:{value:copy.deepcopy(next_iter_dict) for value in rvars[index].values}}\n",
        "            return iter_dis\n",
        "\n",
        "    def query_cpt(self, query_var, sample, prob):\n",
        "        current_val = 0\n",
        "        for item in query_var.cpt[query_var.denotion].items():\n",
        "            prob_dict = item[1]\n",
        "            for p_item in query_var.parents or []:\n",
        "                prob_dict = prob_dict[p_item.denotion][sample[p_item.topo_index-1]]\n",
        "            if (current_val+prob_dict) > prob:\n",
        "                sample.append(item[0])\n",
        "                return\n",
        "            current_val += prob_dict\n",
        "        return \n",
        "    \n",
        "    def enum_ask(self, query, evidence):\n",
        "        query_list = [[(rv.denotion,value) for value in rv.values] for rv in query+evidence]\n",
        "        query_combinations = self.combinations(len(query+evidence), query_list)\n",
        "        distribution = self.build_distribution(query+evidence, 0)\n",
        "        self.call_enum_all(query_combinations, distribution)\n",
        "        self.normalize(distribution, query_combinations, query)\n",
        "        return distribution\n",
        "    \n",
        "    def call_enum_all(self, query_combinations, distribution):\n",
        "        for query in query_combinations:\n",
        "            prob_dis = distribution \n",
        "            query_items = list(query.items())\n",
        "            for i in range(len(query)-1):\n",
        "                prob_dis = prob_dis[query_items[i][0]][query_items[i][1]]\n",
        "            i+=1\n",
        "            prob_dis[query_items[i][0]][query_items[i][1]] = self.enum_all(query)\n",
        "        return \n",
        "    \n",
        "    def enum_all(self, query):\n",
        "        marginilaise_vars = [rv for rv in self.random_vars if rv.denotion not in query.keys()]\n",
        "        marginalize_query_list = [[(rv.denotion,value) for value in rv.values] for rv in marginilaise_vars]\n",
        "        marg_query_comb = self.combinations(len(marginilaise_vars), marginalize_query_list)\n",
        "        new_queries = [copy.deepcopy(dict(query.items()|marg_query.items())) for marg_query in marg_query_comb]\n",
        "        total_prob = 0\n",
        "        for que in new_queries:\n",
        "            marg_prob = 1\n",
        "            for rv in self.random_vars:\n",
        "                prob = rv.cpt[rv.denotion][que[rv.denotion]]\n",
        "                for parent in rv.parents or []:\n",
        "                    prob = prob[parent.denotion][que[parent.denotion]]\n",
        "                marg_prob = marg_prob*prob\n",
        "            total_prob += marg_prob\n",
        "        return total_prob\n",
        "\n",
        "    def normalize(self, distribution, query_combinations, query_vars):\n",
        "        query_vars = [var.denotion for var in query_vars]\n",
        "        normalize_dict = {}\n",
        "        for comb in query_combinations:\n",
        "            key = ''\n",
        "            value = distribution\n",
        "            for item in comb.items():\n",
        "                value = value[item[0]][item[1]]\n",
        "                if not item[0] in query_vars:\n",
        "                    key += str(item[0])+str(item[1])\n",
        "            if key not in normalize_dict.keys():\n",
        "                normalize_dict[key] = value\n",
        "            else:\n",
        "                normalize_dict[key] += value\n",
        "        for comb in query_combinations:\n",
        "            key = ''\n",
        "            value = distribution\n",
        "            items = list(comb.items())\n",
        "            for i in range(len(items)-1):\n",
        "                value = value[items[i][0]][items[i][1]]\n",
        "                if not items[i][0] in query_vars:\n",
        "                    key += str(items[i][0])+str(items[i][1])\n",
        "            i+=1\n",
        "            key += str(items[i][0])+str(items[i][1])\n",
        "            value[items[i][0]][items[i][1]] = (value[items[i][0]][items[i][1]]/normalize_dict[key])\n",
        "\n",
        "    def generate_sample(self, required_values={}):\n",
        "        samples = []\n",
        "        top_order = {node.topo_index-1:node.denotion for node in self.random_vars}\n",
        "        while len(samples)!=1000:\n",
        "            sample = []\n",
        "            flag = True\n",
        "            for rv in self.random_vars:\n",
        "                random_num = random.uniform(0,1)\n",
        "                self.query_cpt(rv, sample, random_num)\n",
        "            if required_values:\n",
        "                for index in range(len(sample)):\n",
        "                    if required_values[top_order[index]] and sample[index] not in required_values[top_order[index]]:\n",
        "                        flag = False\n",
        "                        break\n",
        "            if flag:\n",
        "                samples.append(sample)\n",
        "        return samples\n",
        "    \n",
        "    def query_prior_sample(self, queries):\n",
        "        samples = self.generate_sample()\n",
        "        top_order = {node.denotion:node.topo_index-1 for node in self.random_vars}\n",
        "        for query in queries:\n",
        "            fav_indices = [_ for _ in range(len(samples))]\n",
        "            for item in query['query'].items():\n",
        "                fav_indices = [_ for _ in fav_indices if samples[_][top_order[item[0]]]==item[1]]\n",
        "            evidence_fav_indices = [_ for _ in range(len(samples))]\n",
        "            if query.get('evidence'):\n",
        "                for item in query['evidence'].items():\n",
        "                    fav_indices = [_ for _ in fav_indices if samples[_][top_order[item[0]]]==item[1]]\n",
        "                    evidence_fav_indices = [_ for _ in evidence_fav_indices if samples[_][top_order[item[0]]]==item[1]]\n",
        "            \n",
        "            prob = (len(fav_indices)+1)/(len(evidence_fav_indices)+1)\n",
        "            print(query, 'probability: ', prob)\n",
        "    \n",
        "    def rejection_sampling(self, queries):\n",
        "        required_values = {rv.denotion:set() for rv in self.random_vars}\n",
        "        for query in queries:\n",
        "            if query.get('evidence'):\n",
        "                for item in query.get('evidence').items():\n",
        "                    required_values[item[0]].add(item[1])\n",
        "        samples = self.generate_sample(required_values)\n",
        "        top_order = {node.denotion:node.topo_index-1 for node in self.random_vars}\n",
        "        for query in queries:\n",
        "            fav_indices = [_ for _ in range(len(samples))]\n",
        "            for item in query['query'].items():\n",
        "                fav_indices = [_ for _ in fav_indices if samples[_][top_order[item[0]]]==item[1]]\n",
        "            evidence_fav_indices = [_ for _ in range(len(samples))]\n",
        "            if query.get('evidence'):\n",
        "                for item in query['evidence'].items():\n",
        "                    fav_indices = [_ for _ in fav_indices if samples[_][top_order[item[0]]]==item[1]]\n",
        "                    evidence_fav_indices = [_ for _ in evidence_fav_indices if samples[_][top_order[item[0]]]==item[1]]\n",
        "            prob = len(fav_indices)/len(evidence_fav_indices)\n",
        "            print(query, 'probability: ', prob)\n",
        "    \n",
        "    def generate_likelihood_samples(self, query):\n",
        "        samples = {}\n",
        "        top_order = {node.denotion:node.topo_index-1 for node in self.random_vars}\n",
        "        index = 0\n",
        "        while index!=1000:\n",
        "            sample = []\n",
        "            weight = 1\n",
        "            for rv in self.random_vars:\n",
        "                if query.get('evidence').get(rv.denotion):\n",
        "                    sample.append(query['evidence'][rv.denotion])\n",
        "                    req_cpt_prob = self.random_vars[top_order[rv.denotion]].cpt[rv.denotion][query.get('evidence').get(rv.denotion)]\n",
        "                    for parent in self.random_vars[top_order[rv.denotion]].parents or []:\n",
        "                        req_cpt_prob = req_cpt_prob[parent.denotion][sample[top_order[parent.denotion]]]\n",
        "                    weight = weight * req_cpt_prob\n",
        "                else:\n",
        "                    random_num = random.uniform(0,1)\n",
        "                    self.query_cpt(rv, sample, random_num)\n",
        "            sample = tuple(sample)\n",
        "            if samples.get(sample):\n",
        "                samples[sample]+=weight\n",
        "            else:\n",
        "                samples[sample] = weight\n",
        "            index+=1\n",
        "        return samples\n",
        "\n",
        "    def likelihood_sampling(self, queries):\n",
        "        top_order = {node.denotion:node.topo_index-1 for node in self.random_vars}\n",
        "        for query in queries:\n",
        "            samples = self.generate_likelihood_samples(query)\n",
        "            total_weights = sum(samples.values())\n",
        "            samples = list(samples.items())\n",
        "            fav_indices = [_ for _ in range(len(samples))]\n",
        "            for item in query['query'].items():\n",
        "                fav_indices = [_ for _ in fav_indices if samples[_][0][top_order[item[0]]]==item[1]]\n",
        "            fav_weights = 0\n",
        "            for index in fav_indices:\n",
        "                fav_weights += samples[index][1]\n",
        "            prob = fav_weights/total_weights\n",
        "            print(query, 'probability: ', prob)\n",
        "\n",
        "    def query_gibbs_cpt(self, query_var, sample, prob, distribution, evidence):\n",
        "        current_val = 0\n",
        "        for item in distribution[query_var.denotion].items():\n",
        "            prob_dict = item[1]\n",
        "            for p_item in evidence or []:\n",
        "                prob_dict = prob_dict[p_item.denotion][sample[p_item.topo_index-1]]\n",
        "            if (current_val+prob_dict) > prob:\n",
        "                sample[query_var.topo_index-1]=item[0]\n",
        "                return\n",
        "            current_val += prob_dict\n",
        "        return \n",
        "            \n",
        "    def generate_gibbs_samples(self, query):\n",
        "        samples = {}\n",
        "        sample = []\n",
        "        for rv in self.random_vars:\n",
        "            if rv.denotion in query.get('evidence'):\n",
        "                sample.append(query['evidence'][rv.denotion])\n",
        "            else:\n",
        "                sample.append(rv.values[random.randint(0, len(rv.values)-1)])\n",
        "        samples[tuple(sample)] = 1\n",
        "        while sum(samples.values())!=1000:\n",
        "            random_rv = self.random_vars[random.randint(0, len(self.random_vars)-1)]\n",
        "            while random_rv.denotion in query.get('evidence').keys():\n",
        "                random_rv = self.random_vars[random.randint(0, len(self.random_vars)-1)]\n",
        "            evidence_vars = [_ for _ in random_rv.parents or []+random_rv.children or []]\n",
        "            evidence_vars += [_ for child in random_rv.children or [] for _ in child.parents if _!=random_rv]\n",
        "            evidence_vars = list(set(evidence_vars))\n",
        "            prob_dist = self.enum_ask([random_rv], evidence_vars)\n",
        "            self.query_gibbs_cpt(random_rv, sample, random.uniform(0,1), prob_dist, evidence_vars)\n",
        "            if tuple(sample) in samples.keys():\n",
        "                samples[tuple(sample)] += 1\n",
        "            else:\n",
        "                samples[tuple(sample)] = 1\n",
        "        return samples\n",
        "\n",
        "    def gibbs_sampling(self, queries):\n",
        "        top_order = {node.denotion:node.topo_index-1 for node in self.random_vars}\n",
        "        for query in queries:\n",
        "            samples = self.generate_gibbs_samples(query)\n",
        "            samples = list(samples.items())\n",
        "            fav_indices = [_ for _ in range(len(samples))]\n",
        "            for item in query['query'].items():\n",
        "                fav_indices = [_ for _ in fav_indices if samples[_][0][top_order[item[0]]]==item[1]]\n",
        "            evidence_fav_indices = [_ for _ in range(len(samples))]\n",
        "            if query.get('evidence'):\n",
        "                for item in query['evidence'].items():\n",
        "                    fav_indices = [_ for _ in fav_indices if samples[_][0][top_order[item[0]]]==item[1]]\n",
        "                    evidence_fav_indices = [_ for _ in evidence_fav_indices if samples[_][0][top_order[item[0]]]==item[1]]\n",
        "            fav_events = 0\n",
        "            for _ in fav_indices:\n",
        "                fav_events += samples[_][1]\n",
        "            evidence_events =0\n",
        "            for _ in evidence_fav_indices:\n",
        "                evidence_events += samples[_][1]\n",
        "            prob = fav_events/evidence_events\n",
        "            print(query, 'probability: ', prob)\n",
        "            \n",
        "def main():\n",
        "    burglary = RandomVariable('Burglary', 'B', [True, False], None, None, None, 1)\n",
        "    earthquake = RandomVariable('Earthquake', 'E', [True, False], None, None, None, 2)\n",
        "    alarm = RandomVariable('Alarm', 'A', [True, False], None, None, None, 3)\n",
        "    johncalls = RandomVariable('JohnCalls', 'J', [True, False], None, None, None, 4)\n",
        "    marycalls = RandomVariable('MaryCalls', 'M', [True, False], None, None, None, 5)\n",
        "    burglary.children = [alarm]\n",
        "    earthquake.children = [alarm]\n",
        "    alarm.children = [johncalls, marycalls]\n",
        "    alarm.parents = [burglary, earthquake]\n",
        "    johncalls.parents = [alarm]\n",
        "    marycalls.parents = [alarm]\n",
        "    nodes = [burglary, earthquake, alarm, johncalls, marycalls]\n",
        "    for node in nodes: node.call_build_distribution()\n",
        "    burglary.cpt['B'][True] = 0.001; burglary.cpt['B'][False] = 0.999\n",
        "    earthquake.cpt['E'][True] = 0.002; earthquake.cpt['E'][False] = 0.998 \n",
        "    alarm.cpt['A'][True]['B'][True]['E'][True] = 0.95 \n",
        "    alarm.cpt['A'][True]['B'][True]['E'][False] = 0.94 \n",
        "    alarm.cpt['A'][True]['B'][False]['E'][True] = 0.29 \n",
        "    alarm.cpt['A'][True]['B'][False]['E'][False] = 0.001\n",
        "    alarm.cpt['A'][False]['B'][True]['E'][True] = 0.05 \n",
        "    alarm.cpt['A'][False]['B'][True]['E'][False] = 0.06 \n",
        "    alarm.cpt['A'][False]['B'][False]['E'][True] = 0.71 \n",
        "    alarm.cpt['A'][False]['B'][False]['E'][False] = 0.999\n",
        "    johncalls.cpt['J'][True]['A'][True] = 0.9\n",
        "    johncalls.cpt['J'][True]['A'][False] = 0.05\n",
        "    johncalls.cpt['J'][False]['A'][True] = 0.1\n",
        "    johncalls.cpt['J'][False]['A'][False] = 0.95\n",
        "    marycalls.cpt['M'][True]['A'][True] = 0.7\n",
        "    marycalls.cpt['M'][True]['A'][False] = 0.01\n",
        "    marycalls.cpt['M'][False]['A'][True] = 0.3\n",
        "    marycalls.cpt['M'][False]['A'][False] = 0.99\n",
        "\n",
        "    bayes_network = BayesianNetwork(nodes)\n",
        "    queries = [{'query':{'B':True}, 'evidence':{'J':True, 'M':True}}]\n",
        "    bayes_network.query_prior_sample(queries)\n",
        "    bayes_network.rejection_sampling(queries)\n",
        "    bayes_network.likelihood_sampling(queries)\n",
        "    bayes_network.gibbs_sampling(queries)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}